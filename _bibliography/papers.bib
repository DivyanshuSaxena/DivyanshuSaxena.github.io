@misc{CanopyEuroSys,
title = {Canopy: Property-Driven Learning for Congestion Control}, 
author = {Chenxi Yang and Divyanshu Saxena and Rohit Dwivedula and Kshiteej Mahajan and Swarat Chaudhuri and Aditya Akella},
year = {2026},
abstract={Learning-based congestion controllers offer better adaptability compared to traditional heuristics. However, the unreliability of learning techniques can cause learning-based controllers to behave poorly, creating a need for formal guarantees. While methods for formally verifying learned congestion controllers exist, these methods offer binary feedback that cannot optimize the controller toward better behavior. We improve this state-of-the-art via Canopy, a new property-driven framework that integrates learning with formal reasoning in the learning loop. Canopy uses novel quantitative certification with an abstract interpreter to guide the training process, rewarding models, and evaluating robust and safe model performance on worst-case inputs. Our evaluation demonstrates that unlike state-of-the-art learned controllers, Canopy-trained controllers provide both adaptability and worst-case reliability across a range of network conditions.},
location = {Edinburgh, Scotland},
abbr = {EuroSys '26},
format = {Conference},
}

@inproceedings{GuardrailsHotOS,
title={How I learned to stop worrying and love learned OS policies},
author={Divyanshu Saxena and Jiayi Chen and Sujay Yadalam and Yeonju Ro and Rohit Dwivedula and Eric Campbell and Aditya Akella and Christopher Rossbach and Michael Swift},
year={2025},
abstract={While machine learning has been adopted across various fields, its ability to outperform traditional heuristics in operating systems is often met with justified skepticism. Concerns about unsafe decisions, opaque debugging processes, and the challenges of integrating ML into the kernel—given its stringent latency constraints and inherent complexity—make practitioners understandably cautious. This paper introduces "Guardrails for the OS", a framework that allows kernel developers to declaratively specify system-level properties and define corrective actions to address property violations. The framework facilitates the compilation of these guardrails into monitors capable of running within the kernel. In this work, we establish the foundation for Guardrails, detailing its core abstractions, examining the problem space, and exploring potential solutions.},
location = {Banff, Canada},
url = {https://dl.acm.org/doi/10.1145/3713082.3730384},
abbr = {HotOS '25},
format = {Workshop},
pdf = {hotos25.pdf},
slides = {hotos25_slides.pdf},
}

@inproceedings{CongoIclr,
title={{CONGO}: Compressive Online Gradient Optimization},
author={Jeremy Carleton and Prathik Vijaykumar and Divyanshu Saxena and Dheeraj Narasimha and Srinivas Shakkottai and Aditya Akella},
year={2025},
booktitle={The Thirteenth International Conference on Learning Representations},
abstract={We address the challenge of zeroth-order online convex optimization where the objective function's gradient exhibits sparsity, indicating that only a small number of dimensions possess non-zero gradients. Our aim is to leverage this sparsity to obtain useful estimates of the objective function's gradient even when the only information available is a limited number of function samples. Our motivation stems from the optimization of large-scale queueing networks that process time-sensitive jobs. Here, a job must be processed by potentially many queues in sequence to produce an output, and the service time at any queue is a function of the resources allocated to that queue. Since resources are costly, the end-to-end latency for jobs must be balanced with the overall cost of the resources used. While the number of queues is substantial, the latency function primarily reacts to resource changes in only a few, rendering the gradient sparse. We tackle this problem by introducing the Compressive Online Gradient Optimization framework which allows compressive sensing methods previously applied to stochastic optimization to achieve regret bounds with an optimal dependence on the time horizon without the full problem dimension appearing in the bound. For specific algorithms, we reduce the samples required per gradient estimate to scale with the gradient's sparsity factor rather than its full dimensionality. Numerical simulations and real-world microservices benchmarks demonstrate CONGO's superiority over gradient descent approaches that do not account for sparsity.},
url={https://openreview.net/forum?id=4BFzTrIjPN},
location = {Singapore},
abbr = {ICLR '25},
format = {Conference},
pdf = {https://openreview.net/pdf?id=4BFzTrIjPN},
}

@inproceedings{Copper,
title = {Copper and Wire: Bridging Expressiveness and Performance for Service Mesh Policies}, 
author = {Divyanshu Saxena and William Zhang and Shankara Pailoor and Isil Dillig and Aditya Akella},
year = {2025},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
location = {Rotterdam, The Netherlands},
abstract = {Distributed microservice applications require a convenient means of controlling L7 communication between services. Service meshes have emerged as a popular approach to achieving this. However, current service mesh frameworks are difficult to use -- they burden developers in realizing even simple communication policies, lack compatibility with diverse dataplanes, and introduce performance and resource overheads. We identify the root causes of these drawbacks and propose a ground-up new mesh architecture that overcomes them. We develop novel abstractions for mesh communication, a new mesh policy language centered on these abstractions to enable expressive policies, and a novel control plane that enables using minimal dataplane resources for policy enforcement. We develop the precise semantics of our language abstractions and demonstrate how our control plane can use them to execute policies correctly and optimally. We build and evaluate a prototype on realistic workloads and policies and open-source production traces. Our results show that complex policies can be specified in up to 6.75X fewer lines, enforced with up to 2.6X smaller tail latencies and up to 39\% fewer CPU resources than today.},
url = {https://doi.org/10.1145/3669940.3707257},
code = {https://github.com/utnslab/wire-mesh},
pdf = {copper.pdf},
doi = {10.1145/3669940.3707257},
abbr = {ASPLOS '25},
format = {Conference},
slides = {copper_slides.pdf},
}

@inproceedings{FM4OS,
author = {Saxena, Divyanshu and Sharma, Nihal and Kim, Donghyun and Dwivedula, Rohit and Chen, Jiayi and Yang, Chenxi and Ravula, Sriram and Hu, Zichao and Akella, Aditya and Angel, Sebastian and Biswas, Joydeep and Chaudhuri, Swarat and Dillig, Isil and Dimakis, Alex and Godfrey, Brighten P and Kim, Daehyeok and Rossbach, Christopher and Wang, Gang},
title = {On a Foundation Model for Operating Systems},
booktitle = {7th Workshop on Machine Learning for Systems. Held at 37th Conference on Neural Information Processing Systems (NeurIPS 2023).},
year = {2023},
ws = {MLSys},
abstract = {This paper lays down the research agenda for a domain-specific foundation model for operating systems (OSes). Our case for a foundation model revolves around the observations that several OS components {such as CPU, memory, and network subsystems} are interrelated and that OS traces offer the ideal dataset for a foundation model to grasp the intricacies of diverse OS components and their behavior in varying environments and workloads. We discuss a wide range of possibilities that then arise, from employing foundation models as policy agents to utilizing them as generators and predictors to assist traditional OS control algorithms. Our hope is that this paper spurs further research into OS foundation models and creating the next generation of operating systems for the evolving computing landscape.},
url = {https://arxiv.org/abs/2312.07813},
pdf = {https://utns.cs.utexas.edu/assets/papers/mlsys23-fm4os.pdf},
abbr = {NeurIPS '23},
format = {Workshop}
}

@inproceedings{YamaSoCC,
author = {Ji, Tao and Saxena, Divyanshu and Stephens, Brent E. and Akella, Aditya},
title = {Yama: Providing Performance Isolation for Black-Box Offloads},
year = {2023},
isbn = {9798400703874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620678.3624792},
doi = {10.1145/3620678.3624792},
abstract = {The sharing of clusters with various on-NIC offloads by high-level entities (users, containers, etc.) has become increasingly common. Performance isolation across these entities is desired because the offloads can become bottlenecks due to the limited capacity of hardware. However, the existing works that provide scheduling and resource management to NIC offloads all require customization of the NIC or offloads, while commodity off-the-shelf NICs and offloads with proprietary implementation have been widely deployed in datacenters. This paper presents Yama, the first solution to enable per-entity isolation in the sharing of such black-box NIC offloads. Yama provides a generic framework that captures a common abstraction to the operation of most offloads, which allows operators to incorporate existing offloads. The framework proactively probes for the performance of the offloads with auxiliary workload and enforces isolation at the initiator side. Yama also accommodates chained offloads. Our evaluation shows that 1) Yama achieves per-entity max-min fairness for various types of offloads and in complicated offload chaining scenarios; 2) Yama quickly converges to changes in equilibrium and 3) Yama adds negligible overhead to application workload.},
booktitle = {Proceedings of the 2023 ACM Symposium on Cloud Computing},
pages = {572-587},
numpages = {16},
keywords = {offload, In-network computing, black-box approach, performance isolation},
location = {Santa Cruz, CA, USA},
series = {SoCC '23},
pdf = {https://dl.acm.org/doi/pdf/10.1145/3620678.3624792},
abbr = {SoCC '23},
format = {Conference}
}

@misc{DirigoArxiv,
title = {Dirigo: Self-scaling Stateful Actors For Serverless Real-time Data Processing}, 
author = {Le Xu and Divyanshu Saxena and Neeraja J. Yadwadkar and Aditya Akella and Indranil Gupta},
year = {2023},
eprint = {2308.03615},
archivePrefix = {arXiv},
primaryClass = {cs.DC},
abstract = {We propose Dirigo, a distributed stream processing service built atop virtual actors.
Dirigo achieves both a high level of resource efficiency and performance isolation driven
by user intent (SLO). To improve resource efficiency, Dirigo adopts a serverless architecture
that enables time-sharing of compute resources among streaming operators, both within and
across applications. Meanwhile, Dirigo improves performance isolation by inheriting the
property of function autoscaling from serverless architecture. Specifically, Dirigo
proposes (i) dual-mode actor, an actor abstraction that dynamically provides orderliness
guarantee for streaming operator during autoscaling and (ii) a data plane scheduling
mechanism, along with its API, that allows scheduling and scaling at the message-level
granularity.},
url = {https://arxiv.org/abs/2308.03615},
pdf = {https://arxiv.org/pdf/2308.03615.pdf},
abbr = {Arxiv},
format = {Preprint}
}

@inproceedings{AppliedShort,
author = {Divyanshu Saxena and William Zhang and Madhav Tummala and Saksham Goel and Aditya Akella},
title = {Towards Efficient Microservice Communication.},
booktitle = {Proceedings of the 5th Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating Algorithms for Distributed Systems. Held in conjunction with PODC},
year = {2023},
isbn = {9798400701283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584684.3597267},
doi = {10.1145/3584684.3597267},
abstract = {Distributed applications on the cloud are being developed
and deployed as microservices as opposed to the monolithic architecture.
Service Meshes have emerged as a way of specifying communication
policies between microservices. Service Meshes have the potential
to abstract the networking requirements of distributed applications
from the application logic. However, current service mesh frameworks
introduce significant performance and resource overheads.
We study the overheads of service meshes and make a case for redesigning
both the control plane and data plane for service meshes.
First, we propose the notion of Application Defined Middleboxes,
which makes it possible for the mesh control planes to reduce the
overheads by optimizing where to implement application policies.
Second, we demonstrate preliminary ideas on accelerating the data
plane to further reduce the overheads.},
articleno = {8},
numpages = {5},
location = {Orlando, FL, USA},
series = {ApPLIED 2023},
pdf = {applied23_short.pdf},
ws = {ApPLIED},
abbr = {PODC '23},
format = {Workshop}
}

@article{OSR23,
author = {Divyanshu Saxena and Tao Ji and Arjun Singhvi and Junaid Khalid and Aditya Akella},
title = {Navigating Performance-Efficiency Tradeoffs in Serverless Computing: Deduplication to the Rescue!},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/3606557.3606564},
doi = {10.1145/3606557.3606564},
abstract = {Navigating the performance and efficiency trade-offs is critical for serverless platforms, where the providers ideally want to give the illusion of warm function startups while maintaining low resource costs. Limited controls, provided via toggling sandboxes between warm and cold states and keepalives, force operators to sacrifice significant resources to achieve good performance. We present Medes, a serverless framework, that allows operators to navigate the trade-off space smoothly. Our approach
takes advantage of the high duplication in warm sandboxes
on serverless platforms to develop a new sandbox state, called
a `dedup state', that is more memory-efficient than the warm
state and faster to restore from than the cold state. We use
innovative techniques to identify redundancy with minimal
overhead, and provide a simple management policy to balance performance and memory. Our evaluation demonstrates
that Medes can provide up to 3.8X better end-to-end latencies
and reduce the number of cold starts by 10-50% against the
state-of-the-art baselines.},
journal = {SIGOPS Operating Systems Review},
month = {jun},
pages = {47-53},
numpages = {7},
pdf = {osr23.pdf},
abbr = {ACM OSR '23},
format = {Short Paper},
}

@inproceedings{Medes,
author = {Divyanshu Saxena and Tao Ji and Arjun Singhvi and Junaid Khalid and Aditya Akella},
title = {Memory Deduplication for Serverless Computing with Medes.},
booktitle={Proceedings of the Seventeenth European Conference on Computer Systems},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Serverless platforms today impose rigid trade-offs between
resource use and user-perceived performance. Limited controls, provided via toggling sandboxes between warm and
cold states and keep-alives, force operators to sacrifice significant resources to achieve good performance. We present a
serverless framework, Medes, that breaks the rigid trade-off
and allows operators to navigate the trade-off space smoothly.
Medes leverages the fact that the warm sandboxes running
on serverless platforms have a high fraction of duplication in
their memory footprints. We exploit these redundant chunks
to develop a new sandbox state, called a dedup state, that
is more memory-efficient than the warm state and faster to
restore from than the cold state. We develop novel mechanisms to identify memory redundancy at minimal overhead
while ensuring that the dedup containers' memory footprint
is small. Finally, we develop a simple sandbox management
policy that exposes a narrow, intuitive interface for operators
to trade-off performance for memory by jointly controlling
warm and dedup sandboxes. Detailed experiments with a
protoformat using real-world serverless workloads demonstrate
that Medes can provide up to 1X-2.75X improvements in the
end-to-end latencies. The benefits of Medes are enhanced in
memory pressure situations, where Medes can provide up to
3.8X improvements in end-to-end latencies. Medes achieves
this by reducing the number of cold starts incurred by 10-50%
against the state-of-the-art baselines.},
url = {https://doi.org/10.1145/3492321.3524272},
code = {https://github.com/DivyanshuSaxena/Medes},
pdf = {medes.pdf},
doi = {10.1145/3492321.3524272},
location = {Rennes, France},
abbr = {EuroSys '22},
format = {Conference},
slides = {medes_slides.pdf},
}

@inproceedings{NSDIPoster,
author = {Divyanshu Saxena and Saksham Goel and William Zhang and Madhav Tummala and Aditya Akella},
title = {Application-tailored Communication with xMesh.},
booktitle={Poster session of NSDI '23: 20th USENIX Symposium on Networked Systems Design and Implementation},
year = {2023},
location = {Boston, MA},
abbr = {NSDI '23},
format = {Poster}
}
